
Resequencing Analysis Using Galaxy

The aim of this exercise is to use the web service Galaxy to perform bioinformatic analyses in a graphical environment. A concept that Galaxy makes use of is the "workflow" - a scheme or sequence of separate analysis tools chained together to produce some kind of output. Some kind of data file, e.g. a genome sequence, is fed into the workflow; tools automatically perform their functions in a predetermined manner, and output files are produced that can be visualised or further analysed.
Producing and disseminating workflows give the scientific community and research partners as much transparency as possible for reproducing results and for modifying the workflow for their particular needs.
Most of the steps we are going to perform are going to use the basic settings of each tool in Galaxy. More detailed control can often be accessed in the tool dialog, and should the need for even more control arise, moving onto working with each tool using the command line interface (CLI).

An online demo version of Galaxy can be found on https://usegalaxy.org

The data we will work with comes from the 1000 Genomes Project. Because whole human genomes are very difficult to work with, we will use only a small portion of the human genome, a little over a megabase from chromosome 17. The program samtools was used to extract the data from the 1000 Genomes ftp site for just this region from all of the individuals from the CEU (CEPH Europeans from Utah) population who were low coverage (2-4x average) whole genome shotgun sequenced using the Illumina sequencing platform. We have 81 low coverage Illumina sequences, plus 63 Illumina exomes, and 15 low coverage 454 samples. There are 55 of these samples that were done both ways.

We will walk through alignment, alignment processing and cleanup, variant calling, and variant filtering.


-------------------------------
Aligning reads with BWA

We will align our data to the reference using BWA, a popular aligner based on the Burrows-Wheeler transform.

Before we can run BWA at all, we need a reference genome, and we need to perform the Burrows-Wheeler transform on the reference and build the associated files. For our exercises, we'll use a reference genome only comprised of human chromosome 17. Specifically, we will use the FASTA-formatted genome file "human_17_v37.fasta".

# Step 1: Choose your input fasta-file as reference genome, e.g. "ref/human_17_v37.fasta"


Running BWA for paired end data is done in multiple steps. First we align each set of reads, then we combine the paired alignments (which also includes a realignment step using a more sensitive algorithm for unplaced mates). Let's start with one chunk of whole genome shotgun sequencing data from individual NA06984.
Both of our fastq read files are in Illumina 1.8+ format, but to continue with our analyses we need to convert the files to Sanger format.

# Step 2: Choose forward read dataset, e.g. "wgs/NA06984.ILLUMINA.low_coverage.17q_1.fq"

# Step 3: Choose reverse read dataset, e.g. "wgs/NA06984.ILLUMINA.low_coverage.17q_2.fq"

# Step 4: "FASTQ Groomer" on data from Step 2.

# Step 5: "FASTQ Groomer" on data from Step 3.


-------------------------------
Merge alignments with BWA

After converting the formats to Sanger, the fastq files are used as input files is the next step, where we first align each strand to the reads in our fastq files, and then merge the two alignments - both using the program BWA, a popular aligner using the Burrows-Wheeler transform.

We also need to add something called read groups to our BAM file, because the tools contained in the tool suite Genome Analysis Toolkit (GATK) is going to need this information. Normally, you would do this one sequencing run at a time, but our data are pulled from multiple runs and merged, because of the way this data was downloaded from 1000 Genomes.

# Step 6: "Map with BWA for Illumina" on data from Step 4 and 5. Uses reference genome data from Step 1.
# Specify read groups in "BWA settings to use; Full parameter list". After specifying read groups, you will need to enter some required fields:
#		Read group identiÔ¨Åer (ID)								e.g. "sample_1"
#		Library name (LB)										e.g. "smp_1_lib"
#		Platform/technology used to produce the reads (PL)		"ILLUMINA"
#		Sample (SM).											e.g. "NA06984"


BWA produces binary .sai files that are processed and merged into .sam, an uncompressed human-readable file format, so we need to convert into binary .bam files using the conversion tool "SAM-to-BAM". SAM-to-BAM sorts and indexes the file automatically, in that the entries are sorted in the order in which the reads align to the chromosome, and indexed so that further tools can access the sorted data without going through the whole file.

# Step 7: "SAM-to-BAM" on data from Step 6. Uses reference genome data from Step 1.


-------------------------------
Alignment cleanup

First, we'll realign locally around potential indels. This is done in two steps: First, we identify possible sites to realign.

# Step 8: "Realigner Target Creator" on data from Step 7. Uses reference genome data from Step 1.


Now we feed our gatk_intervals file back into a different GATK tool to actually perform the realignments. GATK automatically indexes that bam for us.

# Step 9: "Indel Realigner" on data from Step 8. Uses reference genome data from Step 1.


Next, we're going to mark duplicate reads using Picard. The resulting bam file with marked duplicates will be used in the next two steps.

# Step 10: "Mark Duplicate Reads" on data from Step 9.


Finally, we want to perform quality recalibration with GATK. We do this last, because we want all the data to be as clean as possible when we get here. This also happens in two steps. First, we compute all the covariation of quality with various other factors.
We need to feed it our bam file and our ref file. We also need a list of known sites. Otherwise, GATK will think all the real SNPs in our data are errors.We're using calls from 1000 Genomes, which is a good plan for human (although a bit circular in our case). If you are sequencing an organism with few known sites, you could try calling once and then using the most confident variants as known sites (which should remove most of the non-erroneous bases). Failure to remove real SNPs from the recalibration will result in globally lower quality scores. We also give it the name of a csv file we want it to write out containing the covariation data.

# Step 11: "Count Covariates" on data from Step 10. Uses reference genome data from Step 1.
# Under tool option "Covariates to be used in the recalibration", tick these boxes:
# 	- QualityScoreCovariate
#	- CycleCovariate
#	- DinucCovariate

# Step 12: "Table Recalibration" on data from Step 10. Uses recalibration csv from Step 11, and reference genome data from Step 1.


-------------------------------
Call SNP variants

We now want to find any SNPs in our sample, and so we want to generate pileup data, or read coverage, from the cleaned bam file from the former step.

# Step 13: "Generate Pileup" on data from Step 12, and reference genome data from Step 1.
# Make sure pileup output is considered "pileup" data rather than "tabular". Edit the output in your history, and under the "datatypes" tab, set new type
# to "pileup".


The last thing we will do is filter variants so that we only keep variants above a certain quality and a certain amount of read coverage to a certain base in the reference genome.

# Step 14: "Filter pileup" on data from Step 13.
# As in Step 13 - make sure that the datatype of the output is set to "pileup" rather than "tabular".
# This output data is set to be the primary output of this certain workflow, which is done by highlighting and clicking the asterisk (*) after that
# particular output file's entry in the workflow Edit mode.


-------------------------------
Looking at Your Data with IGV

You can now display filtered pileup data using e.g. IGV or UCSC Genome Browser.
